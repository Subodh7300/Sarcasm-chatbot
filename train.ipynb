{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
    "from utils import get_tokenizer, set_seed\n",
    "from gpt_dataset import GPT2Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import time \n",
    "from tqdm import tqdm_notebook, tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=5e-05, seed=42, n_gpu=1, gradient_accumulation_steps=2, batch_size=1, num_workers=4, device=device(type='cuda'), num_train_epochs=1, output_dir='./output', model_dir='data/', max_grad_norm=1.0, data_dir='data/train-balanced-sarcasm.csv')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--n_gpu\",default=1, type=int,  help=\"no of gpu available\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",default=2, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch_size\",default=1, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cpu'), type=torch.device, help=\"torch.device object\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=1, type=int,  help=\"no of epochs of training\")\n",
    "parser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--data_dir\",default='./data', type=str, help=\"location of json dataset.\")\n",
    "# parser.add_argument(\"--ids_file\",default='./data', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([\"--device\", \"cuda\", \"--data_dir\", \"data/train-balanced-sarcasm.csv\", \"--model_dir\", \"data/\"])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, tokenizer, train_dataset, ignore_index):\n",
    "    writer = SummaryWriter('./logs')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dl = DataLoader(\n",
    "        train_dataset, \n",
    "        sampler=train_sampler,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers\n",
    "    )\n",
    "    loss_fact = CrossEntropyLoss(ignore_index=ignore_index)\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 100, 80000)\n",
    "    \n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"epochs\")\n",
    "    set_seed(args)\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dl, desc='training')\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = batch['context'], batch['context']\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model = model.to(args.device)\n",
    "            model.train()\n",
    "            logits = model(inputs)[0]\n",
    "            \n",
    "            loc_sep = batch['loc_sep']\n",
    "            shifted_logits = logits[:, loc_sep:-1, :].contiguous()\n",
    "            shifted_labels = labels[:, loc_sep+1:].contiguous()\n",
    "            \n",
    "            loss = loss_fact(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_labels.view(-1))\n",
    "            loss /= args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            if (step+1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "                print(\"Loss: \", loss.item(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17888/17888 [00:04<00:00, 3605.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = GPT2Dataset(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "ignore_index = tokenizer.pad_token_id\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  1691793548.2192638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\Projects\\Sarcasm Chatbot\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c81fa8f8de4da7846c64daa72966ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15d112a4af94cabbc3df1cbd2c5c45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/17888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\Projects\\Sarcasm Chatbot\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  51.353858947753906\n",
      "\n",
      "Loss:  56.93653106689453\n",
      "\n",
      "Loss:  49.18165969848633\n",
      "\n",
      "Loss:  46.95329284667969\n",
      "\n",
      "Loss:  56.562740325927734\n",
      "\n",
      "Loss:  39.54401779174805\n",
      "\n",
      "Loss:  52.501548767089844\n",
      "\n",
      "Loss:  38.652687072753906\n",
      "\n",
      "Loss:  53.08421325683594\n",
      "\n",
      "Loss:  46.54054260253906\n",
      "\n",
      "Loss:  48.969730377197266\n",
      "\n",
      "Loss:  45.39422607421875\n",
      "\n",
      "Loss:  47.49380874633789\n",
      "\n",
      "Loss:  32.578643798828125\n",
      "\n",
      "Loss:  49.13037872314453\n",
      "\n",
      "Loss:  43.82064437866211\n",
      "\n",
      "Loss:  33.50526809692383\n",
      "\n",
      "Loss:  30.307722091674805\n",
      "\n",
      "Loss:  53.80451583862305\n",
      "\n",
      "Loss:  53.27412796020508\n",
      "\n",
      "Loss:  40.14712905883789\n",
      "\n",
      "Loss:  36.830081939697266\n",
      "\n",
      "Loss:  36.907432556152344\n",
      "\n",
      "Loss:  39.81113052368164\n",
      "\n",
      "Loss:  33.867610931396484\n",
      "\n",
      "Loss:  37.34580993652344\n",
      "\n",
      "Loss:  32.081932067871094\n",
      "\n",
      "Loss:  34.3828010559082\n",
      "\n",
      "Loss:  20.223665237426758\n",
      "\n",
      "Loss:  22.06003189086914\n",
      "\n",
      "Loss:  24.696008682250977\n",
      "\n",
      "Loss:  24.050020217895508\n",
      "\n",
      "Loss:  22.766794204711914\n",
      "\n",
      "Loss:  22.20594596862793\n",
      "\n",
      "Loss:  12.083234786987305\n",
      "\n",
      "Loss:  12.368781089782715\n",
      "\n",
      "Loss:  16.92934799194336\n",
      "\n",
      "Loss:  13.300006866455078\n",
      "\n",
      "Loss:  3.909280300140381\n",
      "\n",
      "Loss:  8.802480697631836\n",
      "\n",
      "Loss:  10.163712501525879\n",
      "\n",
      "Loss:  4.968037128448486\n",
      "\n",
      "Loss:  26.722667694091797\n",
      "\n",
      "Loss:  12.488387107849121\n",
      "\n",
      "Loss:  7.283629894256592\n",
      "\n",
      "Loss:  14.3646240234375\n",
      "\n",
      "Loss:  9.023380279541016\n",
      "\n",
      "Loss:  5.478046417236328\n",
      "\n",
      "Loss:  3.1150448322296143\n",
      "\n",
      "Loss:  4.143628120422363\n",
      "\n",
      "Loss:  2.537607431411743\n",
      "\n",
      "Loss:  9.001901626586914\n",
      "\n",
      "Loss:  3.87414288520813\n",
      "\n",
      "Loss:  6.001087188720703\n",
      "\n",
      "Loss:  5.597591400146484\n",
      "\n",
      "Loss:  2.7450478076934814\n",
      "\n",
      "Loss:  4.222922325134277\n",
      "\n",
      "Loss:  3.7310497760772705\n",
      "\n",
      "Loss:  3.651921510696411\n",
      "\n",
      "Loss:  3.6075923442840576\n",
      "\n",
      "Loss:  3.308587074279785\n",
      "\n",
      "Loss:  8.928077697753906\n",
      "\n",
      "Loss:  4.952130317687988\n",
      "\n",
      "Loss:  4.118308067321777\n",
      "\n",
      "Loss:  4.259296894073486\n",
      "\n",
      "Loss:  4.3630781173706055\n",
      "\n",
      "Loss:  4.424145698547363\n",
      "\n",
      "Loss:  3.8009352684020996\n",
      "\n",
      "Loss:  2.597221851348877\n",
      "\n",
      "Loss:  3.0833375453948975\n",
      "\n",
      "Loss:  5.186324596405029\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart time: \u001b[39m\u001b[39m\"\u001b[39m, start)\n\u001b[1;32m----> 4\u001b[0m train(args, model, tokenizer, train_data, ignore_index)\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, tokenizer, train_dataset, ignore_index)\u001b[0m\n\u001b[0;32m     34\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m     35\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 36\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), args\u001b[39m.\u001b[39;49mmax_grad_norm)\n\u001b[0;32m     37\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\WorkSpace\\Projects\\Sarcasm Chatbot\\venv\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:46\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m     44\u001b[0m first_device \u001b[39m=\u001b[39m grads[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice\n\u001b[0;32m     45\u001b[0m grouped_grads: Dict[Tuple[torch\u001b[39m.\u001b[39mdevice, torch\u001b[39m.\u001b[39mdtype], List[List[Tensor]]] \\\n\u001b[1;32m---> 46\u001b[0m     \u001b[39m=\u001b[39m _group_tensors_by_device_and_dtype([[g\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]])  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m norm_type \u001b[39m==\u001b[39m inf:\n\u001b[0;32m     49\u001b[0m     norms \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mto(first_device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]\n",
      "File \u001b[1;32md:\\WorkSpace\\Projects\\Sarcasm Chatbot\\venv\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m     44\u001b[0m first_device \u001b[39m=\u001b[39m grads[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice\n\u001b[0;32m     45\u001b[0m grouped_grads: Dict[Tuple[torch\u001b[39m.\u001b[39mdevice, torch\u001b[39m.\u001b[39mdtype], List[List[Tensor]]] \\\n\u001b[1;32m---> 46\u001b[0m     \u001b[39m=\u001b[39m _group_tensors_by_device_and_dtype([[g\u001b[39m.\u001b[39;49mdetach() \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]])  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m norm_type \u001b[39m==\u001b[39m inf:\n\u001b[0;32m     49\u001b[0m     norms \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mto(first_device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_seed(args)\n",
    "start = time.time()\n",
    "print(\"Start time: \", start)\n",
    "train(args, model, tokenizer, train_data, ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained model...\n"
     ]
    }
   ],
   "source": [
    "print('Saving trained model...')\n",
    "model_file = os.path.join(\"data\", 'model_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(len(train_data),args.num_train_epochs))\n",
    "config_file = os.path.join(\"data\", 'config_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(len(train_data),args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'data/model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/vocab.json', 'data/merges.txt')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_vocabulary('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./data/model_1.pt', map_location=torch.device('cpu'))\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"you're right, but you should have worked hard for it, you know you need to work harder to make a living\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_reply(model, tokenizer, tokenizer.encode(tokenizer.eos_token+\"This cannot be true\", return_tensors='pt').to('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPT3 generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=5e-05, seed=42, n_gpu=1, gradient_accumulation_steps=2, batch_size=1, num_workers=4, device=device(type='cuda'), num_train_epochs=1, output_dir='./output', model_dir='./data/model_3.pt', max_grad_norm=1.0, data_dir='./data/dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--n_gpu\",default=1, type=int,  help=\"no of gpu available\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",default=2, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch_size\",default=1, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), type=torch.device, help=\"torch.device object\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=1, type=int,  help=\"no of epochs of training\")\n",
    "parser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--data_dir\",default='./data', type=str, help=\"location of json dataset.\")\n",
    "# parser.add_argument(\"--ids_file\",default='./data', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([\"--device\", \"cuda\", \"--data_dir\", \"./data/dataset.csv\", \"--model_dir\", \"./data/model_3.pt\"])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175672/175672 [00:41<00:00, 4243.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = CustomDataset(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(172177, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_3 = get_tokenizer()\n",
    "ignore_index = tokenizer_3.pad_token_id\n",
    "\n",
    "model_3 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_3.resize_token_embeddings(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  1692021104.152762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72611172735a4138947a50093014b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139a117a34194a1bbce22e727fd72441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/172177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WorkSpace\\Projects\\Sarcasm Chatbot\\venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:265: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  60.52371597290039\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart time: \u001b[39m\u001b[39m\"\u001b[39m, start)\n\u001b[1;32m----> 4\u001b[0m train(args, model_3, tokenizer_3, train_data, ignore_index)\n",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, tokenizer, train_dataset, ignore_index)\u001b[0m\n\u001b[0;32m     35\u001b[0m loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m     36\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 37\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), args\u001b[39m.\u001b[39;49mmax_grad_norm)\n\u001b[0;32m     38\u001b[0m tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_seed(args)\n",
    "start = time.time()\n",
    "print(\"Start time: \", start)\n",
    "train(args, model_3, tokenizer_3, train_data, ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = torch.load('data/Model_3/model_3.pt')\n",
    "tokenizer_3 = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sorry, this is a long sentence. edit. sorry. my grammar is too long, sorry'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_reply(model_3, tokenizer_3, tokenizer_3.encode(tokenizer_3.eos_token+\"How are you\", return_tensors='pt').to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
